{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Gurobi, JuMP\n",
    "using CSV, DataFrames\n",
    "using LinearAlgebra, Random, Statistics\n",
    "using PyPlot\n",
    "Random.seed!(15095); # Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CSV.read(\"data/final_data.csv\", DataFrame);\n",
    "train = data[data.Year .< 2014, :]\n",
    "test = data[data.Year .>= 2014, :]\n",
    "test = dropmissing(test)\n",
    "lnr = IAI.OptKNNImputationLearner()\n",
    "train = IAI.fit_transform!(lnr, train)\n",
    "\n",
    "X_train, y_train = Matrix(select(train, Not([\"Tm\", \"Year\", \"WinLoss\"]))), train[:, \"WinLoss\"]\n",
    "X_test, y_test = Matrix(select(test, Not([\"Tm\", \"Year\", \"WinLoss\"]))), test[:, \"WinLoss\"];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = names(select(train, Not([\"Tm\", \"Year\", \"WinLoss\"])))\n",
    "features_transformed = [transf * f for f in features for transf in [\"\", \"squared_\", \"sqrt_\", \"exp_\", \"log_\"]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = (X_train .- minimum(vcat(X_train, X_test), dims = 1)) ./ (maximum(vcat(X_train, X_test), dims = 1) - minimum(vcat(X_train, X_test), dims = 1))\n",
    "X_test_norm = (X_test .- minimum(vcat(X_train, X_test), dims = 1)) ./ (maximum(vcat(X_train, X_test), dims = 1) - minimum(vcat(X_train, X_test), dims = 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that transforms the matrix input matrix by adding non-linear transformations to all columns\n",
    "function transform_data(X, eps = 0.1)\n",
    "\n",
    "    n, p = size(X) # Store the matrix dimension\n",
    "    # Initiate the transformed matrix with the first feature\n",
    "    Xt = hcat(X[:,1], X[:,1].^2, X[:,1].^0.5, exp.(X[:,1]), log.(X[:,1] .+ eps))\n",
    "\n",
    "    # Loop for all other features and concatenate the transformations to the matrix\n",
    "    for i = 2:p\n",
    "        Xt = hcat(Xt, X[:,i], X[:,i].^2, X[:,i].^0.5, exp.(X[:,i]), log.(X[:,i] .+ eps))\n",
    "    end\n",
    "\n",
    "    return Xt;\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that returns a list of feature couples whose correlation exceeds rho\n",
    "function pairwise_correlation(X, rho)\n",
    "\n",
    "    _, p = size(X) # Store the number of features\n",
    "    cor_matrix = cor(X); # Compute the correlation matrix\n",
    "    cor_variables = [] # Initialize an empty list to store all pairwise correlated features couples\n",
    "\n",
    "    for i = 1:p \n",
    "        for j = 1:i-1 # Loop under all values of the lower diagonal of the correlation matrix\n",
    "\n",
    "            if abs(cor_matrix[i,j]) > rho\n",
    "                # If the correlation coefficient of features i and j exceeds rho, append the couple to the list\n",
    "                push!(cor_variables, (i,j))\n",
    "            end\n",
    "            \n",
    "        end\n",
    "    end\n",
    "\n",
    "    return cor_variables;\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "function holistic_regression(X, y, rho, lambda, k, M)\n",
    "\n",
    "    # Create the transformed matrix\n",
    "    X_transformed = transform_data(X);\n",
    "    n, p = size(X) # Store the input matrix size\n",
    "    _, p_tilde = size(X_transformed) # Set the number of features in the transformed matrix\n",
    "    augmentation = Int(p_tilde / p)\n",
    "    # Compute the list of correlated feature couples\n",
    "    HC = pairwise_correlation(X_transformed, rho)\n",
    "\n",
    "    # Create JuMP model\n",
    "    model = Model(Gurobi.Optimizer)\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", 0)\n",
    "\n",
    "    # Introduce model variables\n",
    "    @variable(model, Beta[1:p_tilde])\n",
    "    @variable(model, a[1:p_tilde] >= 0)\n",
    "    @variable(model, z[1:p_tilde], binary = true)\n",
    "\n",
    "    # Robustness term constraint linearization\n",
    "    @constraint(model,[j = 1:p_tilde], Beta[j] <= a[j])\n",
    "    @constraint(model,[j = 1:p_tilde], -Beta[j] <= a[j])\n",
    "\n",
    "    # Big-M integer constraint for sparsity\n",
    "    @constraint(model,[j = 1:p_tilde], Beta[j] <= M*z[j])\n",
    "    @constraint(model,[j = 1:p_tilde], Beta[j] >= -M*z[j])\n",
    "    # Sparsity constraint\n",
    "    @constraint(model, sum(z[j] for j = 1:p_tilde) <= k)\n",
    "\n",
    "    # Non-linear transformations constraint\n",
    "    @constraint(model, [j = 1:p], sum(z[augmentation*(j-1)+i] for i = 1:augmentation) <= 1)\n",
    "\n",
    "    # Pairwise collinearity\n",
    "    @constraint(model, [i = 1:length(HC)], z[HC[i][1]] + z[HC[i][2]] <= 1)\n",
    "\n",
    "    # Implement the objective function of the problem\n",
    "    @objective(model, Min, sum((y[i] - sum(X_transformed[i,j]*Beta[j] for j=1:p_tilde))^2 for i = 1:n) + lambda * sum(a[j] for j = 1:p_tilde))\n",
    "\n",
    "    # Solve the optimization problem\n",
    "    optimize!(model);\n",
    "\n",
    "    Beta, z = JuMP.value.(Beta), Int.(round.(JuMP.value.(z)))\n",
    "\n",
    "    y_pred = X_transformed * Beta\n",
    "    r2 = 1 - sum((y_pred .- y).^2) / sum((0.5 .- y).^2)\n",
    "    print(\"Training R2: $(r2)\")\n",
    "\n",
    "    return Beta, z, r2\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-08-18\n",
      "Training R2: 0.5419875935374558"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4281358608642384"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the holisitic regression problem parameters\n",
    "rho = 0.7\n",
    "lambda = 0.01\n",
    "k = 10\n",
    "M = 100\n",
    "\n",
    "# Compute the holistic regression model\n",
    "Beta, z, _ = holistic_regression(X_train_norm, y_train, rho, lambda, k, M)\n",
    "X_test_transformed = transform_data(X_test_norm);\n",
    "y_pred = X_test_transformed * Beta;\n",
    "r2 = 1 - sum((y_pred .- y_test).^2) / sum((0.5 .- y_test).^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_PTS_1 :  0.11896170343603811\n",
      "exp_PER_1 :  -0.033132947448919745\n",
      "exp_VORP_2 :  0.15276310405270352\n",
      "exp_PER_2 :  -0.10649886279099867\n",
      "squared_VORP_3 :  0.09487749003455642\n",
      "log_SRS_2 :  -0.041901136332918844\n",
      "exp_SRS_1 :  0.29046405233427147\n",
      "log_Rest :  -0.021893815451698922\n",
      "log_distLB :  0.018561385424663183\n",
      "squared_distUB :  -0.04668325465148927\n"
     ]
    }
   ],
   "source": [
    "for zidx in findall(z .== 1)\n",
    "    println(features_transformed[zidx], \" :  \", Beta[zidx])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8158d229fe7301e640acb43d488c0efce3cd8ada31ff414dc1808c070c07cefa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
